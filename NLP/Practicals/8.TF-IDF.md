### Overview of TF-IDF in Natural Language Processing (NLP)

TF-IDF, which stands for **Term Frequency-Inverse Document Frequency**, is a widely used method in Natural Language Processing (NLP) for converting textual data into numerical vectors. This transformation is essential for applying machine learning models to text data, as these models require numerical inputs.

### Core Concepts of TF-IDF

1. **Term Frequency (TF):**
   - Measures how often a word appears in a sentence or document relative to the total number of words in that document.
   - Formula:  
     $
     TF = \frac{\text{Number of occurrences of the word in the document}}{\text{Total number of words in the document}}
     $

2. **Inverse Document Frequency (IDF):**
   - Measures the importance of a word across the entire corpus (all documents or sentences).
   - Words that appear frequently across all documents (e.g., stop words like "the," "and") are less informative and are assigned lower weights.
   - Formula:  
     $
     IDF = \log_e\left(\frac{\text{Total number of documents}}{\text{Number of documents containing the word}}\right)
     $

3. **TF-IDF Score:**
   - Combines TF and IDF to assign a weight to each word in a document. This weight reflects the importance of the word in the document while considering its relevance in the entire corpus.
   - Formula:  
     $
     TF\text{-}IDF = TF \times IDF
     $

### Example Walkthrough

#### Input Sentences

- Sentence 1 (S1): "good boy"
- Sentence 2 (S2): "good girl"
- Sentence 3 (S3): "boy girl good"

#### Steps

1. **Vocabulary Creation:**
   Identify the unique words (vocabulary) across all sentences:  
   $
   \text{Vocabulary} = \{\text{good, boy, girl}\}
   $

2. **Calculate Term Frequency (TF):**
   For each word in each sentence, compute the term frequency using the formula above.

   Example for Sentence 1:  
   - TF(good) = $\frac{1}{2}$  
   - TF(boy) = $\frac{1}{2}$  
   - TF(girl) = $\frac{0}{2}$

   Similarly, compute for Sentence 2 and Sentence 3.

3. **Calculate Inverse Document Frequency (IDF):**
   Compute IDF for each word using the formula.

   Example for "good":  
   - "good" appears in all three sentences (S1, S2, S3), so:  
     $
     IDF(good) = \log_e\left(\frac{3}{3}\right) = 0
     $

   Similarly, compute for "boy" and "girl."

4. **Combine TF and IDF to Calculate TF-IDF:**
   Multiply the term frequency (TF) by the inverse document frequency (IDF) for each word in each sentence.

5. **Convert Sentences into Vectors:**
   The TF-IDF scores for each word in the vocabulary form a vector for each sentence.

   Example for Sentence 1 (S1):  
   - TF-IDF(good) = $\frac{1}{2} \times 0 = 0$  
   - TF-IDF(boy) = $\frac{1}{2} \times \text{IDF(boy)}$  
   - TF-IDF(girl) = $\frac{0}{2} \times \text{IDF(girl)} = 0$

   Repeat for S2 and S3.

### Significance of TF-IDF

1. **Importance Weighting:**  
   TF-IDF helps highlight words that are more relevant to specific documents while downplaying common but uninformative words.

2. **Sparse Representations:**  
   Converts textual data into sparse vectors, suitable for machine learning algorithms.

3. **Improved Performance:**  
   TF-IDF often performs better than simple frequency-based approaches like Bag of Words (BoW), as it accounts for word relevance across documents.

In the next session, we will explore **advantages, disadvantages**, and **practical applications** of TF-IDF, along with a comparison to other vectorization techniques such as **word embeddings (e.g., Word2Vec, GloVe)**.### Overview of TF-IDF in Natural Language Processing (NLP)

TF-IDF, which stands for **Term Frequency-Inverse Document Frequency**, is a widely used method in Natural Language Processing (NLP) for converting textual data into numerical vectors. This transformation is essential for applying machine learning models to text data, as these models require numerical inputs.

### Core Concepts of TF-IDF

1. **Term Frequency (TF):**
   - Measures how often a word appears in a sentence or document relative to the total number of words in that document.
   - Formula:  
     $
     TF = \frac{\text{Number of occurrences of the word in the document}}{\text{Total number of words in the document}}
     $

2. **Inverse Document Frequency (IDF):**
   - Measures the importance of a word across the entire corpus (all documents or sentences).
   - Words that appear frequently across all documents (e.g., stop words like "the," "and") are less informative and are assigned lower weights.
   - Formula:  
     $
     IDF = \log_e\left(\frac{\text{Total number of documents}}{\text{Number of documents containing the word}}\right)
     $

3. **TF-IDF Score:**
   - Combines TF and IDF to assign a weight to each word in a document. This weight reflects the importance of the word in the document while considering its relevance in the entire corpus.
   - Formula:  
     $
     TF\text{-}IDF = TF \times IDF
     $

### Example Walkthrough

#### Input Sentences

- Sentence 1 (S1): "good boy"
- Sentence 2 (S2): "good girl"
- Sentence 3 (S3): "boy girl good"

#### Steps

1. **Vocabulary Creation:**
   Identify the unique words (vocabulary) across all sentences:  
   $
   \text{Vocabulary} = \{\text{good, boy, girl}\}
   $

2. **Calculate Term Frequency (TF):**
   For each word in each sentence, compute the term frequency using the formula above.

   Example for Sentence 1:  
   - TF(good) = $\frac{1}{2}$  
   - TF(boy) = $\frac{1}{2}$  
   - TF(girl) = $\frac{0}{2}$

   Similarly, compute for Sentence 2 and Sentence 3.

3. **Calculate Inverse Document Frequency (IDF):**
   Compute IDF for each word using the formula.

   Example for "good":  
   - "good" appears in all three sentences (S1, S2, S3), so:  
     $
     IDF(good) = \log_e\left(\frac{3}{3}\right) = 0
     $

   Similarly, compute for "boy" and "girl."

4. **Combine TF and IDF to Calculate TF-IDF:**
   Multiply the term frequency (TF) by the inverse document frequency (IDF) for each word in each sentence.

5. **Convert Sentences into Vectors:**
   The TF-IDF scores for each word in the vocabulary form a vector for each sentence.

   Example for Sentence 1 (S1):  
   - TF-IDF(good) = $\frac{1}{2} \times 0 = 0$  
   - TF-IDF(boy) = $\frac{1}{2} \times \text{IDF(boy)}$  
   - TF-IDF(girl) = $\frac{0}{2} \times \text{IDF(girl)} = 0$

   Repeat for S2 and S3.

### Significance of TF-IDF

1. **Importance Weighting:**  
   TF-IDF helps highlight words that are more relevant to specific documents while downplaying common but uninformative words.

2. **Sparse Representations:**  
   Converts textual data into sparse vectors, suitable for machine learning algorithms.

3. **Improved Performance:**  
   TF-IDF often performs better than simple frequency-based approaches like Bag of Words (BoW), as it accounts for word relevance across documents.

In the next session, we will explore **advantages, disadvantages**, and **practical applications** of TF-IDF, along with a comparison to other vectorization techniques such as **word embeddings (e.g., Word2Vec, GloVe)**.
