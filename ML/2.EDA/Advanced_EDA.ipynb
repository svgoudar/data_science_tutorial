{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f78f68a-41b9-4f88-ac24-2ce60f616bb2",
   "metadata": {},
   "source": [
    "To perform **Exploratory Data Analysis (EDA)**, combined with **data preprocessing**, **data cleaning**, handling **imbalance**, and thorough **visualizations**, we can follow a structured pipeline. Here's an outline of the process, followed by code snippets.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Load the Data**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Overview of the dataset\n",
    "print(df.head())       # First few rows\n",
    "print(df.info())       # Column types and non-null counts\n",
    "print(df.describe())   # Statistical summary\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Cleaning**\n",
    "### **a. Handle Missing Values**\n",
    "```python\n",
    "# Check missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize missing values\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Data Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Handle missing values\n",
    "# Option 1: Drop rows/columns with missing values\n",
    "df = df.dropna()  # Drop rows\n",
    "# OR\n",
    "df = df.drop(columns=['irrelevant_column'])  # Drop a column\n",
    "\n",
    "# Option 2: Impute missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')  # Options: 'mean', 'median', 'most_frequent'\n",
    "df['column_name'] = imputer.fit_transform(df[['column_name']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Handle Duplicates**\n",
    "```python\n",
    "# Check for duplicates\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Data Transformation**\n",
    "### **a. Scaling Continuous Variables**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()  # Use MinMaxScaler() for normalization\n",
    "df[['col1', 'col2']] = scaler.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "### **b. Encoding Categorical Variables**\n",
    "```python\n",
    "# One-hot encoding\n",
    "df = pd.get_dummies(df, columns=['categorical_column'], drop_first=True)\n",
    "\n",
    "# Label encoding (for ordinal data)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['ordinal_column'] = le.fit_transform(df['ordinal_column'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Handling Imbalanced Data**\n",
    "### **a. Check Class Distribution**\n",
    "```python\n",
    "# Visualize class imbalance\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Print class proportions\n",
    "print(df['target'].value_counts(normalize=True))\n",
    "```\n",
    "\n",
    "### **b. Address Imbalance**\n",
    "#### **Option 1: Oversampling**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X, y = df.drop(columns='target'), df['target']\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "#### **Option 2: Undersampling**\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "#### **Option 3: Class Weights in Model**\n",
    "```python\n",
    "# Example: Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Visualization**\n",
    "### **a. Pairplot**\n",
    "```python\n",
    "sns.pairplot(df, hue='target')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **b. Correlation Matrix**\n",
    "```python\n",
    "corr = df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **c. Box Plot for Outliers**\n",
    "```python\n",
    "sns.boxplot(x='target', y='numerical_column', data=df)\n",
    "plt.title(\"Boxplot of Numerical Column by Target\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **d. Distribution Plot**\n",
    "```python\n",
    "for col in ['col1', 'col2']:\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### **e. Class Imbalance After Handling**\n",
    "```python\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title(\"Class Distribution After Handling Imbalance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Feature Engineering**\n",
    "### **a. Create New Features**\n",
    "```python\n",
    "# Example: Create interaction terms\n",
    "df['new_feature'] = df['col1'] * df['col2']\n",
    "```\n",
    "\n",
    "### **b. Feature Selection**\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(selector.get_support())  # Selected features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Handle Outliers**\n",
    "### **a. Detect Outliers**\n",
    "```python\n",
    "# Using IQR\n",
    "Q1 = df['numerical_column'].quantile(0.25)\n",
    "Q3 = df['numerical_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['numerical_column'] < lower_bound) | (df['numerical_column'] > upper_bound)]\n",
    "print(\"Outliers:\", outliers)\n",
    "```\n",
    "\n",
    "### **b. Remove or Cap Outliers**\n",
    "```python\n",
    "# Option 1: Remove\n",
    "df = df[~((df['numerical_column'] < lower_bound) | (df['numerical_column'] > upper_bound))]\n",
    "\n",
    "# Option 2: Cap\n",
    "df['numerical_column'] = np.clip(df['numerical_column'], lower_bound, upper_bound)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Save the Cleaned Data**\n",
    "```python\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see detailed code for any specific step or a summary of the cleaned dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827ea9e-7441-4037-a7b1-ff89e6205669",
   "metadata": {},
   "source": [
    "Here's a more comprehensive and exhaustive EDA and data preprocessing pipeline, covering additional topics such as time series handling, feature scaling techniques, anomaly detection, handling multicollinearity, feature importance, balancing datasets, and advanced visualization techniques. Each topic is elaborated below:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Advanced Data Cleaning**\n",
    "### **a. Handling Inconsistent Data**\n",
    "```python\n",
    "# Example: Standardizing categorical values\n",
    "df['category_column'] = df['category_column'].str.lower().str.strip()\n",
    "df['category_column'] = df['category_column'].replace({'cat': 'category', 'categ.': 'category'})\n",
    "\n",
    "# Removing irrelevant or unnecessary columns\n",
    "df = df.drop(columns=['irrelevant_column'])\n",
    "```\n",
    "\n",
    "### **b. Handling Zero or Negative Values**\n",
    "- Sometimes zero or negative values are invalid for certain columns like age, income, etc.\n",
    "```python\n",
    "# Replace zero or negative values with NaN, then handle NaN values\n",
    "df['column'] = df['column'].replace(0, np.nan)\n",
    "df['column'] = df['column'].replace(df['column'] < 0, np.nan)\n",
    "df['column'] = df['column'].fillna(df['column'].median())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Dealing with Time-Series Data**\n",
    "### **a. Parse and Process Datetime Features**\n",
    "```python\n",
    "# Convert a column to datetime\n",
    "df['date_column'] = pd.to_datetime(df['date_column'])\n",
    "\n",
    "# Extract useful features from datetime\n",
    "df['year'] = df['date_column'].dt.year\n",
    "df['month'] = df['date_column'].dt.month\n",
    "df['day'] = df['date_column'].dt.day\n",
    "df['day_of_week'] = df['date_column'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x > 4 else 0)\n",
    "```\n",
    "\n",
    "### **b. Handle Missing Time Periods**\n",
    "```python\n",
    "# Reindex with a complete time range\n",
    "df = df.set_index('date_column').resample('D').mean()  # Resample daily\n",
    "df = df.fillna(method='ffill')  # Forward-fill missing values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Feature Scaling**\n",
    "### **a. Log Transformation for Right-Skewed Data**\n",
    "```python\n",
    "df['log_transformed'] = np.log1p(df['column'])  # log1p handles log(0) by adding 1\n",
    "```\n",
    "\n",
    "### **b. Robust Scaling (Less Sensitive to Outliers)**\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "df[['col1', 'col2']] = scaler.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Advanced Handling of Categorical Data**\n",
    "### **a. Frequency Encoding**\n",
    "```python\n",
    "freq_encoding = df['categorical_column'].value_counts(normalize=True)\n",
    "df['freq_encoded'] = df['categorical_column'].map(freq_encoding)\n",
    "```\n",
    "\n",
    "### **b. Target Encoding**\n",
    "```python\n",
    "# Mean encoding based on the target\n",
    "mean_target = df.groupby('categorical_column')['target'].mean()\n",
    "df['target_encoded'] = df['categorical_column'].map(mean_target)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Detecting and Removing Multicollinearity**\n",
    "### **a. Correlation Matrix**\n",
    "```python\n",
    "# Detecting multicollinearity using correlation matrix\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **b. Variance Inflation Factor (VIF)**\n",
    "```python\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for numerical columns\n",
    "X = df.drop(columns=['target'])  # Exclude target column\n",
    "vif = pd.DataFrame()\n",
    "vif['Variable'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)\n",
    "```\n",
    "- Drop columns with high VIF (>10) to reduce multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Handling Imbalanced Datasets**\n",
    "### **a. Combining SMOTE with Tomek Links**\n",
    "- SMOTE generates synthetic data, and Tomek Links removes borderline examples.\n",
    "```python\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smote_tomek = SMOTETomek()\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Anomaly Detection**\n",
    "### **a. Z-Score Method**\n",
    "```python\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate Z-scores\n",
    "df['z_score'] = zscore(df['column'])\n",
    "\n",
    "# Filter out anomalies (Z-score > 3)\n",
    "anomalies = df[df['z_score'].abs() > 3]\n",
    "df = df[df['z_score'].abs() <= 3]\n",
    "```\n",
    "\n",
    "### **b. Isolation Forest**\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.05)  # Specify contamination percentage\n",
    "df['anomaly'] = iso.fit_predict(df[['col1', 'col2']])\n",
    "df = df[df['anomaly'] == 1]  # Keep only non-anomalous data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Feature Engineering**\n",
    "### **a. Polynomial Features**\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "### **b. Interaction Terms**\n",
    "```python\n",
    "df['interaction_term'] = df['col1'] * df['col2']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Advanced Visualizations**\n",
    "### **a. Pairplot with KDE**\n",
    "```python\n",
    "sns.pairplot(df, hue='target', kind='kde')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **b. FacetGrid**\n",
    "```python\n",
    "g = sns.FacetGrid(df, col=\"categorical_column\", hue=\"target\", height=4)\n",
    "g.map(sns.histplot, 'numerical_column', kde=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **c. Violin Plot**\n",
    "```python\n",
    "sns.violinplot(x='categorical_column', y='numerical_column', hue='target', data=df, split=True)\n",
    "plt.title(\"Violin Plot\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Outlier Detection and Treatment**\n",
    "### **a. Mahalanobis Distance**\n",
    "- Useful for multivariate outliers.\n",
    "```python\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Calculate Mahalanobis distance\n",
    "cov_matrix = np.cov(df[['col1', 'col2']].values.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "mean_values = df[['col1', 'col2']].mean(axis=0)\n",
    "\n",
    "df['mahalanobis_dist'] = df[['col1', 'col2']].apply(lambda x: mahalanobis(x, mean_values, inv_cov_matrix), axis=1)\n",
    "\n",
    "# Filter out rows with high Mahalanobis distance\n",
    "threshold = df['mahalanobis_dist'].quantile(0.99)\n",
    "df = df[df['mahalanobis_dist'] < threshold]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Splitting the Dataset**\n",
    "### **a. Train-Test Split**\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### **b. Cross-Validation Split**\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Saving Processed Data**\n",
    "### **a. Save Preprocessed Data**\n",
    "```python\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This pipeline ensures that all aspects of data preprocessing, cleaning, imbalance handling, anomaly detection, and feature engineering are covered. Let me know if you'd like to dive deeper into any of these steps or apply them to a specific dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f3b27-5e15-4832-b62e-2ef053158171",
   "metadata": {},
   "source": [
    "Here's a **comprehensive and exhaustive pipeline** for EDA, data preprocessing, cleaning, handling imbalance, outlier treatment, visualization, and advanced feature engineering, covering all possible techniques and ensuring every relevant Python package is included. Let’s break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Basic EDA**\n",
    "### **a. Importing Necessary Libraries**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from scipy.stats import zscore, chi2, normaltest\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from numpy.linalg import inv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n",
    "```\n",
    "\n",
    "### **b. Data Overview**\n",
    "```python\n",
    "# Basic overview\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Data types\n",
    "print(df.dtypes)\n",
    "```\n",
    "\n",
    "### **c. Checking Target Class Distribution**\n",
    "```python\n",
    "# For classification problems\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title(\"Target Class Distribution\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Handling Missing Values**\n",
    "### **a. Simple Imputation**\n",
    "```python\n",
    "# Numerical columns\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df['numerical_column'] = num_imputer.fit_transform(df[['numerical_column']])\n",
    "\n",
    "# Categorical columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['categorical_column'] = cat_imputer.fit_transform(df[['categorical_column']])\n",
    "```\n",
    "\n",
    "### **b. KNN Imputation**\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[['col1', 'col2']] = knn_imputer.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Outlier Detection and Treatment**\n",
    "### **a. Z-Score Method**\n",
    "```python\n",
    "df['z_score'] = zscore(df['numerical_column'])\n",
    "df = df[df['z_score'].abs() <= 3]  # Retain only non-outliers\n",
    "```\n",
    "\n",
    "### **b. IQR Method**\n",
    "```python\n",
    "Q1 = df['numerical_column'].quantile(0.25)\n",
    "Q3 = df['numerical_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[(df['numerical_column'] >= Q1 - 1.5 * IQR) & (df['numerical_column'] <= Q3 + 1.5 * IQR)]\n",
    "```\n",
    "\n",
    "### **c. Mahalanobis Distance**\n",
    "```python\n",
    "# Covariance and mean\n",
    "cov_matrix = np.cov(df[['col1', 'col2']].values.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "mean_values = df[['col1', 'col2']].mean(axis=0)\n",
    "\n",
    "# Calculate Mahalanobis distance\n",
    "df['mahalanobis_dist'] = df[['col1', 'col2']].apply(lambda x: mahalanobis(x, mean_values, inv_cov_matrix), axis=1)\n",
    "threshold = chi2.ppf((1 - 0.01), df[['col1', 'col2']].shape[1])  # 99% confidence\n",
    "df = df[df['mahalanobis_dist'] <= threshold]\n",
    "```\n",
    "\n",
    "### **d. Isolation Forest**\n",
    "```python\n",
    "iso = IsolationForest(contamination=0.05)\n",
    "df['anomaly'] = iso.fit_predict(df[['col1', 'col2']])\n",
    "df = df[df['anomaly'] == 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Feature Engineering**\n",
    "### **a. Creating New Features**\n",
    "```python\n",
    "df['interaction_term'] = df['col1'] * df['col2']  # Interaction term\n",
    "df['ratio'] = df['col1'] / (df['col2'] + 1e-5)    # Avoid division by zero\n",
    "```\n",
    "\n",
    "### **b. Polynomial Features**\n",
    "```python\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(df[['col1', 'col2']])\n",
    "df_poly = pd.DataFrame(poly_features, columns=['col1^2', 'col1*col2', 'col2^2'])\n",
    "```\n",
    "\n",
    "### **c. PCA for Dimensionality Reduction**\n",
    "```python\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df[['col1', 'col2', 'col3']])\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Feature Scaling**\n",
    "### **a. Standard Scaling**\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "df[['col1', 'col2']] = scaler.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "### **b. Min-Max Scaling**\n",
    "```python\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[['col1', 'col2']] = min_max_scaler.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "### **c. Robust Scaling**\n",
    "```python\n",
    "robust_scaler = RobustScaler()\n",
    "df[['col1', 'col2']] = robust_scaler.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Handling Categorical Data**\n",
    "### **a. Label Encoding**\n",
    "```python\n",
    "le = LabelEncoder()\n",
    "df['categorical_column'] = le.fit_transform(df['categorical_column'])\n",
    "```\n",
    "\n",
    "### **b. One-Hot Encoding**\n",
    "```python\n",
    "df = pd.get_dummies(df, columns=['categorical_column'], drop_first=True)\n",
    "```\n",
    "\n",
    "### **c. Frequency and Target Encoding**\n",
    "```python\n",
    "# Frequency Encoding\n",
    "freq_encoding = df['categorical_column'].value_counts(normalize=True)\n",
    "df['freq_encoded'] = df['categorical_column'].map(freq_encoding)\n",
    "\n",
    "# Target Encoding\n",
    "mean_target = df.groupby('categorical_column')['target'].mean()\n",
    "df['target_encoded'] = df['categorical_column'].map(mean_target)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Handling Data Imbalance**\n",
    "### **a. SMOTE**\n",
    "```python\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### **b. SMOTE-Tomek**\n",
    "```python\n",
    "smote_tomek = SMOTETomek()\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Multicollinearity Detection**\n",
    "### **a. Correlation Matrix**\n",
    "```python\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **b. Variance Inflation Factor (VIF)**\n",
    "```python\n",
    "X = df.drop(columns=['target'])\n",
    "vif = pd.DataFrame()\n",
    "vif['Variable'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Visualization Techniques**\n",
    "### **a. Pairplot**\n",
    "```python\n",
    "sns.pairplot(df, hue='target')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **b. Distribution Plot**\n",
    "```python\n",
    "sns.histplot(df['numerical_column'], kde=True)\n",
    "plt.title(\"Distribution of Column\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **c. Violin Plot**\n",
    "```python\n",
    "sns.violinplot(x='categorical_column', y='numerical_column', hue='target', data=df, split=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **d. Boxplot**\n",
    "```python\n",
    "sns.boxplot(x='categorical_column', y='numerical_column', data=df)\n",
    "plt.title(\"Boxplot\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Splitting Data**\n",
    "### **a. Train-Test Split**\n",
    "```python\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### **b. Cross-Validation**\n",
    "```python\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This pipeline ensures every aspect of **EDA, preprocessing, visualization, and advanced data preparation techniques** is covered. Let me know if you'd like to explore a specific step further or see its application on a dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c649f83-ef2c-4f19-bec2-515e8370a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
