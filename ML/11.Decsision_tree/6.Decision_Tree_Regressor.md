# Decision Tree Regressor Explanation and Variance Reduction

### Video Script Outline

#### **Introduction**

- Briefly introduce Decision Tree Regressor.
- Recap the completed Decision Tree Classifier concepts, like entropy, Gini impurity, pre/post pruning.
- Highlight the transition to regression problems, where the output is a **continuous feature**.
- Example: Predicting salary based on experience and career gap.

---

#### **Conceptual Overview**

1. **Regression Problem Statement**:
   - Output feature is continuous.
   - Example: Salary prediction using features like experience and career gap.

2. **Splits in Decision Tree Regressor**:
   - Unlike classification, regression requires different metrics for split evaluation since output is continuous.
   - In classification, metrics like entropy, Gini impurity, and information gain are used.
   - For regression, we use **Variance Reduction** as the key criterion.

---

#### **Variance Reduction**

- **Definition**: Measures how well a split reduces the variability (variance) in the target variable.
- Formula for Variance:
  $
  \text{Variance} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \bar{y})^2
  $
  where:
  - $y_i$: Actual target values.
  - $\bar{y}$: Mean of target values.
  - $N$: Number of target values.

- Variance Reduction:
  $
  \text{Variance Reduction} = \text{Variance}_{\text{root}} - \left( \frac{\text{Size of child 1}}{\text{Size of root}} \times \text{Variance}_{\text{child 1}} + \frac{\text{Size of child 2}}{\text{Size of root}} \times \text{Variance}_{\text{child 2}} \right)
  $

---

#### **Step-by-Step Example**

1. **Dataset**: Experience and career gap as input features; salary as output.
   - Sample data: $\{40K, 42K, 52K, 60K, 56K\}$.

2. **Root Node Variance Calculation**:
   - Compute the mean ($\bar{y}$) of target values:
     $
     \bar{y} = \frac{40 + 42 + 52 + 60 + 56}{5} = 50
     $
   - Variance:
     $
     \text{Variance}_{\text{root}} = \frac{1}{5} \left( (40 - 50)^2 + (42 - 50)^2 + (52 - 50)^2 + (60 - 50)^2 + (56 - 50)^2 \right)
     $
     $
     = \frac{1}{5} \left( 100 + 64 + 4 + 100 + 36 \right) = \frac{304}{5} = 60.8
     $

3. **Child Node Variance Calculation**:
   - **Split 1**: Threshold \( \leq 2$, $\{40K, 42K\}$ vs $\{52K, 60K, 56K\}$.
     - Variance of Child 1 ($\text{C1}$):
       $
       \text{C1 Variance} = \frac{1}{2} \left( (40 - 41)^2 + (42 - 41)^2 \right) = 1
       $
     - Variance of Child 2 ($\text{C2}$):
       $
       \text{C2 Variance} = \frac{1}{3} \left( (52 - 56)^2 + (60 - 56)^2 + (56 - 56)^2 \right) = 8
       $

4. **Variance Reduction**:
   - Combine variances of child nodes based on size and calculate variance reduction:
     $
     \text{Variance Reduction} = 60.8 - \left( \frac{2}{5} \times 1 + \frac{3}{5} \times 8 \right)
     $

---

#### **Conclusion**

- Decision Tree Regressor uses **Variance Reduction** to determine the best split.
- Variance Reduction ensures we minimize the error in predicting continuous output values.
- This is similar in principle to minimizing Mean Squared Error (MSE) in regression models.

#### **Next Steps**

- Implement the Decision Tree Regressor in Python.
- Demonstrate the algorithm using a practical dataset.
