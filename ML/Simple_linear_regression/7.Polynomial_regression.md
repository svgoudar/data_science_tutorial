In this video, we are going to discuss a new machine learning algorithm called **Polynomial Regression**.

### Quick Recap of Linear Regression

Before we dive into polynomial regression, let's quickly review **linear regression**:

- In **simple linear regression**, we have one independent variable (let's call it $x$) and one dependent variable ($y$). The goal is to fit a straight line that best represents the data points.
- The equation for simple linear regression is:  
  $y = \beta_0 + \beta_1 x$
- If we have more than one independent variable, we use **multiple linear regression**, where the goal is to fit a plane (in 3D) or a hyperplane (in higher dimensions).

### Why Polynomial Regression?

- Linear regression works well when the relationship between $x$ and $y$ is **linear**.
- However, if the data shows a **non-linear** pattern (like a curve), fitting a straight line will not capture the relationship well, leading to high errors.
  
  For example:
  - If we have data points that form a curve, a straight line won't fit well.
  - The error (difference between the actual values and predicted values) will be high.

- **Polynomial Regression** helps solve this by fitting a curve to the data instead of a straight line.

### Understanding Polynomial Regression

- In polynomial regression, we transform the original features into **polynomial features** (higher powers of the original feature).
- Let's break down the concept of polynomial degrees:
  
  1. **Degree = 0**:  
     $y = \beta_0$  
     This is just a constant value (a horizontal line).
  
  2. **Degree = 1**:  
     $y = \beta_0 + \beta_1 x$  
     This is the same as simple linear regression (a straight line).

  3. **Degree = 2**:  
     $y = \beta_0 + \beta_1 x + \beta_2 x^2$  
     Now we have a **quadratic** equation, which can fit curves like parabolas.

  4. **Degree = 3 and higher**:  
     $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \ldots + \beta_n x^n$  
     As we increase the degree, the model can capture more complex patterns in the data.

### Choosing the Right Degree

- If the degree is too low, the model **underfits** (it cannot capture the complexity of the data).
- If the degree is too high, the model **overfits** (it captures too much noise, fitting the data points too closely).
- Our goal is to find a degree that **balances** fitting the data well without overfitting or underfitting.

### Polynomial Regression with Multiple Features

- So far, we've considered only one input feature ($x$). But if we have multiple features ($x_1, x_2, \ldots$), we can still apply polynomial regression.
- For example, if we have two features $x_1$ and $x_2$:
  - **Degree = 1**:  

    $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$
  - **Degree = 2**:  
    $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2 + \beta_4 x_2^2 + \beta_5 (x_1 \times x_2)$
  - **Degree = 3 and higher**: we can add cubic and higher-order terms for each feature.

### Summary

- Polynomial regression is great for capturing **non-linear** relationships in data.
- We need to experiment with different degrees to find the best fit without overfitting or underfitting.
- In the next part of the video, we'll solve a practical problem using polynomial regression and see how it works in practice.

That's it for now! See you in the next video. Thank you!
