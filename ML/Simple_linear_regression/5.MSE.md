In this video, we're going to discuss three important regression evaluation metrics: **Mean Squared Error (MSE)**, **Mean Absolute Error (MAE)**, and **Root Mean Squared Error (RMSE)**. These metrics help us measure how well our model is performing by focusing on the errors for each data point, unlike **R²** and **Adjusted R²**, which we've already covered in our previous video for assessing the overall fit of the model.

### Introduction to Error Metrics

Let's consider a regression problem where we have a dataset with **years of experience** on the x-axis and **salary** on the y-axis. After training a linear regression model, we get a **best fit line**. This line helps us predict the **salary** based on a given number of years of experience.

For each data point:

- The **actual value** (denoted as $y$) is the true salary.
- The **predicted value** (denoted as $\hat{y}$) is the salary predicted by our model.

The difference between these actual and predicted values is known as the **error**. Our goal is to minimize this error to improve our model's performance. This is where MSE, MAE, and RMSE come into play.

### Mean Squared Error (MSE)

The **Mean Squared Error** is one of the most commonly used metrics to measure the performance of a regression model. Its formula is:

$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$

- $y_i$ is the actual value.
- $\hat{y}_i$ is the predicted value.
- $n$ is the number of data points.

**Key Points:**

- MSE calculates the average squared difference between actual and predicted values.
- The squaring of errors means that larger errors have a disproportionately higher impact, making MSE sensitive to outliers.
- It provides a **quadratic** cost function, which is useful for optimization techniques like **gradient descent**.

#### Advantages of MSE

1. **Differentiable**: The squared error function is differentiable, which means we can use techniques like gradient descent to minimize the error effectively.
2. **Single Global Minima**: Because the MSE is a quadratic function, it forms a **convex curve**, meaning there’s only one global minimum. This prevents the model from getting stuck in local minima during optimization.
3. **Convergence**: Due to its convex nature, models trained using MSE typically converge faster.

#### Disadvantages of MSE

1. **Not Robust to Outliers**: Because errors are squared, larger errors are heavily penalized, making MSE sensitive to outliers. If there’s a data point that’s far away from the others, it will disproportionately affect the MSE, causing the model to shift the best fit line to accommodate this outlier.
2. **Units Issue**: The error value produced by MSE is in the squared units of the output variable. For example, if the target variable (like salary) is in lakhs, the MSE would be in lakhs², which can be harder to interpret directly.

### Mean Absolute Error (MAE)

The **Mean Absolute Error** is another metric for evaluating regression models. Its formula is:

$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$

**Key Points:**

- MAE calculates the average of the absolute differences between actual and predicted values.
- Unlike MSE, MAE doesn’t square the errors, so it’s less sensitive to outliers.
- The error value is in the same unit as the target variable, making it easier to interpret.

#### Advantages of MAE

1. **Robust to Outliers**: MAE treats all errors equally and doesn’t disproportionately penalize larger errors, making it more robust to outliers.
2. **Interpretable Units**: The error is measured in the same units as the original data, making it easier to understand.

#### Disadvantages of MAE

1. **Non-Differentiable at Zero**: The absolute value function is not differentiable at zero, which can complicate the optimization process.
2. **No Convexity Guarantee**: The loss surface of MAE isn’t convex like MSE, which may result in multiple local minima, making optimization harder.

### Root Mean Squared Error (RMSE)

The **Root Mean Squared Error** is essentially the square root of MSE:

$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$

**Key Points:**

- RMSE gives an error value in the same units as the target variable, like MAE.
- It balances between penalizing large errors (like MSE) and providing interpretable results (like MAE).

#### Advantages of RMSE

1. **Penalizes Large Errors**: Like MSE, RMSE penalizes larger errors more significantly, which can be useful if we want to focus on minimizing large deviations.
2. **Interpretable Units**: RMSE is in the same unit as the target variable, making it more interpretable than MSE.

#### Disadvantages of RMSE

1. **Still Sensitive to Outliers**: Although not as sensitive as MSE, RMSE can still be affected by outliers due to the squaring of errors before taking the square root.
2. **Higher Computational Cost**: Calculating the square root can add computational overhead, especially with large datasets.

### Summary of Metrics

| Metric | Formula | Robust to Outliers? | Units | Differentiable? |
|--------|---------|---------------------|-------|-----------------|
| MSE    | $\frac{1}{n} \sum (y_i - \hat{y}_i)^2$ | No  | Squared Units | Yes |
| MAE    | $\frac{1}{n} \sum y_i - \hat{y}_i$|  Yes | Same as target | No at zero |
| RMSE   | $\sqrt{\frac{1}{n} \sum (y_i - \hat{y}_i)^2}$ | No (less than MSE) | Same as target | Yes |

### Conclusion

- Use **MSE** if you want a differentiable loss function and can tolerate sensitivity to outliers.
- Use **MAE** if you need robustness to outliers and interpretability in the same units.
- Use **RMSE** as a balanced approach between MSE and MAE, especially if you want interpretable error values but can tolerate some sensitivity to outliers.

By understanding the strengths and weaknesses of each metric, you can choose the most appropriate one based on your specific regression problem and dataset characteristics.
