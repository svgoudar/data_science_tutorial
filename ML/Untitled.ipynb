{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cee026d-9c64-4938-9ecf-bdbeaa304ae3",
   "metadata": {},
   "source": [
    "To perform **Exploratory Data Analysis (EDA)**, combined with **data preprocessing**, **data cleaning**, handling **imbalance**, and thorough **visualizations**, we can follow a structured pipeline. Here's an outline of the process, followed by code snippets.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Load the Data**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Overview of the dataset\n",
    "print(df.head())       # First few rows\n",
    "print(df.info())       # Column types and non-null counts\n",
    "print(df.describe())   # Statistical summary\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Cleaning**\n",
    "### **a. Handle Missing Values**\n",
    "```python\n",
    "# Check missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize missing values\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Data Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Handle missing values\n",
    "# Option 1: Drop rows/columns with missing values\n",
    "df = df.dropna()  # Drop rows\n",
    "# OR\n",
    "df = df.drop(columns=['irrelevant_column'])  # Drop a column\n",
    "\n",
    "# Option 2: Impute missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')  # Options: 'mean', 'median', 'most_frequent'\n",
    "df['column_name'] = imputer.fit_transform(df[['column_name']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Handle Duplicates**\n",
    "```python\n",
    "# Check for duplicates\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Data Transformation**\n",
    "### **a. Scaling Continuous Variables**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()  # Use MinMaxScaler() for normalization\n",
    "df[['col1', 'col2']] = scaler.fit_transform(df[['col1', 'col2']])\n",
    "```\n",
    "\n",
    "### **b. Encoding Categorical Variables**\n",
    "```python\n",
    "# One-hot encoding\n",
    "df = pd.get_dummies(df, columns=['categorical_column'], drop_first=True)\n",
    "\n",
    "# Label encoding (for ordinal data)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['ordinal_column'] = le.fit_transform(df['ordinal_column'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Handling Imbalanced Data**\n",
    "### **a. Check Class Distribution**\n",
    "```python\n",
    "# Visualize class imbalance\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Print class proportions\n",
    "print(df['target'].value_counts(normalize=True))\n",
    "```\n",
    "\n",
    "### **b. Address Imbalance**\n",
    "#### **Option 1: Oversampling**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X, y = df.drop(columns='target'), df['target']\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "#### **Option 2: Undersampling**\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "#### **Option 3: Class Weights in Model**\n",
    "```python\n",
    "# Example: Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Visualization**\n",
    "### **a. Pairplot**\n",
    "```python\n",
    "sns.pairplot(df, hue='target')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **b. Correlation Matrix**\n",
    "```python\n",
    "corr = df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **c. Box Plot for Outliers**\n",
    "```python\n",
    "sns.boxplot(x='target', y='numerical_column', data=df)\n",
    "plt.title(\"Boxplot of Numerical Column by Target\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **d. Distribution Plot**\n",
    "```python\n",
    "for col in ['col1', 'col2']:\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### **e. Class Imbalance After Handling**\n",
    "```python\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title(\"Class Distribution After Handling Imbalance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Feature Engineering**\n",
    "### **a. Create New Features**\n",
    "```python\n",
    "# Example: Create interaction terms\n",
    "df['new_feature'] = df['col1'] * df['col2']\n",
    "```\n",
    "\n",
    "### **b. Feature Selection**\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(selector.get_support())  # Selected features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Handle Outliers**\n",
    "### **a. Detect Outliers**\n",
    "```python\n",
    "# Using IQR\n",
    "Q1 = df['numerical_column'].quantile(0.25)\n",
    "Q3 = df['numerical_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['numerical_column'] < lower_bound) | (df['numerical_column'] > upper_bound)]\n",
    "print(\"Outliers:\", outliers)\n",
    "```\n",
    "\n",
    "### **b. Remove or Cap Outliers**\n",
    "```python\n",
    "# Option 1: Remove\n",
    "df = df[~((df['numerical_column'] < lower_bound) | (df['numerical_column'] > upper_bound))]\n",
    "\n",
    "# Option 2: Cap\n",
    "df['numerical_column'] = np.clip(df['numerical_column'], lower_bound, upper_bound)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Save the Cleaned Data**\n",
    "```python\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see detailed code for any specific step or a summary of the cleaned dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77531fb-73a9-4804-aff8-fd30e7513026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
