### Support Vector Machines (SVMs) - Mathematical Intuition

#### Introduction

In this video, we will delve into the mathematical intuition behind **Support Vector Machines (SVMs)**. We’ve already introduced the primary aim of SVM in our previous video: to create a **best-fit hyperplane** that maximizes the margin between two classes while satisfying classification constraints.

---

#### Revisiting the Equation of a Line

Let’s start with a **2D plane** where we have two featur    s, $x_1$ and $x_2$. The equation of a straight line can be written as:  

$w_1x_1 + w_2x_2 + b = 0$
This can also be expressed compactly as:
$w^\top x + b = 0$  
where:  

- $w = [w_1, w_2]^\top$ is the **weight vector**.  
- $x = [x_1, x_2]^\top$ is the feature vector.  
- $b$ is the **bias term**.  

If the line passes through the origin, $b = 0$, reducing the equation to $w^\top x = 0$. However, when $b \neq 0$, it shifts the line away from the origin.

---

#### Understanding the Weight Vector ($w$)

The weight vector, $w$, is **perpendicular** to the hyperplane. Its direction determines how the hyperplane is oriented. The distance from any point to this hyperplane depends on $w$ and $b$, as shown below.

---

#### Distance Between a Point and the Hyperplane

To calculate the distance ($d$) from a point $x_i$ to the hyperplane, the formula is:  
$d = \frac{|w^\top x_i + b|}{\|w\|}$

Key observations:  

1. Points **above the hyperplane** (in the positive class) yield a positive distance.  
2. Points **below the hyperplane** (in the negative class) yield a negative distance.  

---

#### Marginal Planes and Support Vectors

To classify points, SVM introduces two **marginal planes**:  

1. $w^\top x + b = +1$ for the positive class.  
2. $w^\top x + b = -1$ for the negative class.  

Points lying **on** these marginal planes are called **support vectors**. These are critical because they define the margins and influence the hyperplane’s position.

---

#### The Margin

The **margin** is the perpendicular distance between the two marginal planes:
$\text{Margin} = \frac{2}{\|w\|}$

Our goal in SVM is to **maximize this margin** while ensuring all points are classified correctly.

---

#### Classification Constraints

For correctly classified points:  

1. If $y_i = +1$ (positive class):  
   $w^\top x_i + b \geq +1$
2. If $y_i = -1$ (negative class):  
   $w^\top x_i + b \leq -1$  
Combining both conditions, we get:  
$y_i \cdot (w^\top x_i + b) \geq 1 \quad \forall i$

---

#### Objective Function

To maximize the margin, we minimize $\|w\|^2$ (or equivalently $\frac{1}{2}\|w\|^2$) under the constraints:  
$y_i \cdot (w^\top x_i + b) \geq 1 \quad \forall i$

---

#### Summary

In this video, we discussed:  

1. The **equation of the hyperplane** and its geometric interpretation.  
2. The role of the **weight vector** and bias in defining the hyperplane.  
3. **Support vectors** and **marginal planes** that influence the hyperplane.  
4. The derivation of the **margin** and the constraints for correct classification.  

In the next video, we will explore how to optimize this objective function using techniques like **Lagrange multipliers** and the **dual formulation** of SVM. Thank you!
