{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "861f5c74-4fc3-4f35-89fd-73eb82d8a43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall opencv-python-headless torch mediapipe pandas opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a17ca-93bf-43a5-b174-57fb27ed0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce3b4a0-35ee-49c5-856c-7be3208655df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# Step 1: Load YOLOv8 model for object detection\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Pre-trained YOLO model\n",
    "\n",
    "# Step 2: Initialize MediaPipe Pose for pose estimation\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Step 3: Define helper functions\n",
    "def detect_cyclists_and_runners(image):\n",
    "    \"\"\"\n",
    "    Detect cyclists and runners in the input image using YOLO and MediaPipe Pose.\n",
    "    Args:\n",
    "        image: Input image.\n",
    "    Returns:\n",
    "        Annotated image with cyclists and runners highlighted.\n",
    "    \"\"\"\n",
    "    # Convert image to RGB for MediaPipe\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Step 3.1: Run YOLO detection\n",
    "    results = model(rgb_image)\n",
    "    detections = results.pandas().xyxy[0]  # YOLO detections as DataFrame\n",
    "\n",
    "    for _, detection in detections.iterrows():\n",
    "        # Extract bounding box and label\n",
    "        xmin, ymin, xmax, ymax, label, confidence = (\n",
    "            int(detection['xmin']),\n",
    "            int(detection['ymin']),\n",
    "            int(detection['xmax']),\n",
    "            int(detection['ymax']),\n",
    "            detection['name'],\n",
    "            detection['confidence'],\n",
    "        )\n",
    "\n",
    "        # Crop the detected person\n",
    "        if label == 'person':\n",
    "            person_roi = rgb_image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            # Step 3.2: Use MediaPipe Pose for pose estimation\n",
    "            results_pose = pose.process(person_roi)\n",
    "            if results_pose.pose_landmarks:\n",
    "                # Analyze pose to classify as running or cycling\n",
    "                annotated_image = analyze_pose(image, xmin, ymin, xmax, ymax, results_pose.pose_landmarks)\n",
    "\n",
    "    return image\n",
    "\n",
    "def analyze_pose(image, xmin, ymin, xmax, ymax, landmarks):\n",
    "    \"\"\"\n",
    "    Analyze pose landmarks to determine if the person is running or cycling.\n",
    "    Args:\n",
    "        image: Original image.\n",
    "        xmin, ymin, xmax, ymax: Bounding box coordinates.\n",
    "        landmarks: Pose landmarks.\n",
    "    Returns:\n",
    "        Annotated image with activity labeled.\n",
    "    \"\"\"\n",
    "    # Extract key landmarks (e.g., knees, ankles, wrists)\n",
    "    left_knee = landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "    right_knee = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "    left_ankle = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "    right_ankle = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "\n",
    "    # Example heuristic for classification:\n",
    "    # - Cycling: Knees move in circular motion, feet near pedals (higher vertical range)\n",
    "    # - Running: High vertical motion of knees, regular stride motion\n",
    "\n",
    "    knee_distance = abs(left_knee.y - right_knee.y)\n",
    "    ankle_distance = abs(left_ankle.y - right_ankle.y)\n",
    "\n",
    "    label = \"Running\" if knee_distance > 0.2 else \"Cycling\"\n",
    "\n",
    "    # Draw the bounding box and label\n",
    "    cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "    cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Step 4: Run on a sample image\n",
    "if __name__ == \"__main__\":\n",
    "    # Load input image\n",
    "    image_path = \"street_scene.jpg\"  # Replace with your image path\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Detect cyclists and runners\n",
    "    output_image = detect_cyclists_and_runners(image)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow(\"Output\", output_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9155edb5-80b9-4ef3-beec-3756a52a381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "C:\\Users\\sangouda\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16abc39-af31-4dbe-ba8b-4230761af2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
