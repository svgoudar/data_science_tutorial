
## **Introduction:**

- In KNN, we calculate the distance between a query point and all points in the dataset, leading to a time complexity of **O(n)**.
- To optimize this, we use **variants like k-d trees** and **ball trees**.

---

## **Understanding k-d Trees:**

1. **Definition:**  
   A k-d tree (k-dimensional tree) is a **binary tree** used to organize points in a k-dimensional space for efficient search operations like nearest neighbor queries.

2. **Motivation:**  
   Instead of calculating distances for all points, the k-d tree divides the space into hierarchical regions, reducing the number of points to check.

---

## **Step-by-Step Construction:**

1. **Data Points:**  
   Let's assume these 6 points in 2D space (features F1 and F2):  
  $
   (7, 2), (5, 4), (9, 6), (2, 3), (4, 7), (8, 1)
  $

2. **Step 1 - Splitting by F1 (x-axis):**
   - Sort the points by F1:$2, 4, 5, 7, 8, 9$.
   - **Median (pivot):**$7$ (splits the data).  
   - Draw a vertical line at $x = 7$, dividing the space into **Region 1** ($x < 7$) and **Region 2** ($x \geq 7$).

3. **Step 2 - Splitting by F2 (y-axis):**
   - Focus on Region 1 ($(5,4), (2,3), (4,7)$) and Region 2 ($(9,6), (8,1)$) separately.  
   - For **Region 1**:
     - Sort points by F2:$3, 4, 7$.
     - **Median (pivot):**$4$ (splits the data).  
     - Draw a horizontal line at$y = 4$.
   - For **Region 2**:
     - Sort points by F2:$1, 6$.
     - **Median (pivot):**$6$ (splits the data).  
     - Draw a horizontal line at$y = 6$.

4. **Recursive Splits:**  
   Repeat the process alternately along F1 and F2 for the resulting subregions until only one point remains in each region.

---

## **Resulting Tree Structure:**

The binary tree looks like this:

```
       (7,2)
      /     \
   (5,4)   (9,6)
   /   \   /   \
(2,3) (4,7) (8,1)
```

---

## **Advantages of k-d Tree:**

1. **Faster Query:**  
   Instead of searching all points, the query is limited to relevant regions, reducing the time complexity to **O(log n)** in balanced trees.

2. **Efficient Search:**  
   By dividing the space hierarchically, unnecessary comparisons are avoided.

---

## Ball Tree Variant of KNN

The Ball Tree is another efficient method to reduce the time complexity of K-Nearest Neighbors (KNN) search. It organizes data points into a hierarchical structure of nested hyperspheres (balls) to facilitate efficient nearest-neighbor queries.

---

### **How Ball Tree Works**

1. **Tree Construction**:
   - The Ball Tree is built by recursively dividing the dataset into smaller subsets, each enclosed in a hypersphere or "ball."
   - At each node, data points are partitioned into two subsets based on their distance from a carefully chosen centroid.
   - The centroid is usually the mean or median of the points in the subset.

2. **Splitting Criterion**:
   - For a given set of points:
     1. Compute the centroid of the points.
     2. Find the point farthest from the centroid (let’s call it point A).
     3. Then, find the point farthest from point A (let’s call it point B).
     4. Use the line connecting points A and B to divide the points into two subsets, forming two child nodes.

3. **Recursive Process**:
   - Continue the process for each subset until the number of points in a subset falls below a specified threshold or cannot be split further.

---

### **Querying with Ball Tree**

When searching for the nearest neighbors using a Ball Tree:

1. Start at the root node.
2. Check if the query point lies within the hypersphere of the current node.
   - If it does, explore the child nodes.
   - If not, prune the branch, as points in this branch cannot be the nearest neighbors.
3. Continue this process until you reach leaf nodes (where individual points are stored).
4. Compute distances only for the points in the relevant leaf nodes.

---

### **Advantages of Ball Tree**

- **Efficient Search**:
  - Unlike K-d Trees, which use axis-aligned splits, Ball Trees consider the true geometric distance between points. This makes them more effective for higher-dimensional data.
- **Reduced Distance Computations**:
  - By pruning irrelevant branches, the Ball Tree significantly reduces the number of distance calculations required during nearest-neighbor queries.
- **Handles High-Dimensional Data**:
  - While not as scalable as specialized methods like Approximate Nearest Neighbors (ANN), Ball Trees perform better than K-d Trees in moderately high-dimensional spaces.

---

### **Comparison: K-d Tree vs. Ball Tree**

| Feature             | K-d Tree                | Ball Tree              |
|---------------------|-------------------------|------------------------|
| **Splitting Rule**  | Axis-aligned splits     | Distance-based splits  |
| **Structure**       | Binary tree             | Hierarchical hyperspheres |
| **Efficiency**      | Best for low dimensions | Better for higher dimensions |
| **Data Type**       | Works for numeric data  | Works for any metric space |

---

### **Use Case Example**

#### Data

- 2D points:$(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2)$

#### Steps

1. Compute the centroid of the points, say$(6.5, 4.16)$.
2. Determine the farthest point from the centroid, say$(2, 3)$.
3. Find the point farthest from$(2, 3)$, say$(9, 6)$.
4. Split the dataset into two groups:
   - Points closer to$(2, 3)$.
   - Points closer to$(9, 6)$.
5. Continue recursively for each subset.

---

### **Key Points**

- Ball Tree is an effective alternative to K-d Tree, especially for high-dimensional data or data with irregular distributions.
- It uses distance-based splits, making it more flexible than axis-aligned splits in K-d Trees.
- Its hierarchical structure enables efficient nearest-neighbor search by pruning irrelevant regions.
